device cuda n_gpu 1 distributed training False
***** Running training *****
  Batch size = 4
  Num steps = 1816
step: 10 | train loss: 0.011066799983382225 | train acc 0.7235543131828308
step: 20 | train loss: 0.010673802345991135 | train acc 0.7070844769477844
step: 30 | train loss: 0.010227642022073269 | train acc 0.732262372970581
step: 40 | train loss: 0.010452254675328732 | train acc 0.7364864945411682
step: 50 | train loss: 0.01006552204489708 | train acc 0.7142857313156128
step: 60 | train loss: 0.011091356165707111 | train acc 0.6924119591712952
step: 70 | train loss: 0.011968093924224377 | train acc 0.6529160737991333
step: 80 | train loss: 0.010652881115674973 | train acc 0.7261055707931519
step: 90 | train loss: 0.009236181154847145 | train acc 0.7468706369400024
step: 100 | train loss: 0.009990047663450241 | train acc 0.7115902900695801
step: 110 | train loss: 0.011181928217411041 | train acc 0.6977715492248535
step: 120 | train loss: 0.010250559076666832 | train acc 0.6930422782897949
step: 130 | train loss: 0.011743288487195969 | train acc 0.7099236845970154
step: 140 | train loss: 0.011097060516476631 | train acc 0.6821378469467163
step: 150 | train loss: 0.010582544840872288 | train acc 0.71074378490448
step: 160 | train loss: 0.010946777649223804 | train acc 0.6993103623390198
step: 170 | train loss: 0.010627934709191322 | train acc 0.692307710647583
step: 180 | train loss: 0.009121453389525414 | train acc 0.732262372970581
step: 190 | train loss: 0.009930203668773174 | train acc 0.7348066568374634
step: 200 | train loss: 0.011467966251075268 | train acc 0.703592836856842
step: 210 | train loss: 0.010064387694001198 | train acc 0.7102425694465637
step: 220 | train loss: 0.009864309802651405 | train acc 0.7312414646148682
step: 230 | train loss: 0.010152101516723633 | train acc 0.7150610685348511
step: 240 | train loss: 0.012312321923673153 | train acc 0.6525307893753052
step: 250 | train loss: 0.00939400028437376 | train acc 0.7264276742935181
step: 260 | train loss: 0.010431742295622826 | train acc 0.7649918794631958
step: 270 | train loss: 0.01032742578536272 | train acc 0.7120566964149475
step: 280 | train loss: 0.010391713120043278 | train acc 0.7194950580596924
step: 290 | train loss: 0.008996344171464443 | train acc 0.7466124892234802
step: 300 | train loss: 0.009670442901551723 | train acc 0.7267759442329407
step: 310 | train loss: 0.010073034092783928 | train acc 0.7332382798194885
step: 320 | train loss: 0.011701717972755432 | train acc 0.698113203048706
step: 330 | train loss: 0.011724317446351051 | train acc 0.7079510688781738
step: 340 | train loss: 0.009551276452839375 | train acc 0.7577388882637024
step: 350 | train loss: 0.009627770632505417 | train acc 0.75
step: 360 | train loss: 0.00905994325876236 | train acc 0.7270340919494629
step: 370 | train loss: 0.010067001916468143 | train acc 0.7266483902931213
step: 380 | train loss: 0.009469002485275269 | train acc 0.7303988933563232
step: 390 | train loss: 0.013397907838225365 | train acc 0.7123287916183472
step: 400 | train loss: 0.010146884247660637 | train acc 0.7156084179878235
step: 410 | train loss: 0.01027906034141779 | train acc 0.7222222685813904
step: 420 | train loss: 0.009073570370674133 | train acc 0.75
step: 430 | train loss: 0.008020872250199318 | train acc 0.79347825050354
step: 440 | train loss: 0.008549434132874012 | train acc 0.7636364102363586
step: 450 | train loss: 0.010306321084499359 | train acc 0.7443609237670898
step: 460 | train loss: 0.010791593231260777 | train acc 0.7482615113258362
step: 470 | train loss: 0.010005581192672253 | train acc 0.7399165630340576
step: 480 | train loss: 0.009331426583230495 | train acc 0.7303523421287537
step: 490 | train loss: 0.009619973599910736 | train acc 0.7188754677772522
step: 500 | train loss: 0.009661934338510036 | train acc 0.7375504374504089
step: 510 | train loss: 0.009048067964613438 | train acc 0.7630137205123901
step: 520 | train loss: 0.009238294325768948 | train acc 0.7326732873916626
step: 530 | train loss: 0.009847193025052547 | train acc 0.7482615113258362
step: 540 | train loss: 0.008837406523525715 | train acc 0.750325083732605
step: 550 | train loss: 0.00959792546927929 | train acc 0.731034517288208
step: 560 | train loss: 0.008355595171451569 | train acc 0.767346978187561
step: 570 | train loss: 0.00935695506632328 | train acc 0.767978310585022
step: 580 | train loss: 0.009625589475035667 | train acc 0.7982608675956726
step: 590 | train loss: 0.01055153738707304 | train acc 0.7336122989654541
step: 600 | train loss: 0.009706719778478146 | train acc 0.727642297744751
step: 610 | train loss: 0.009584997780621052 | train acc 0.741978645324707
step: 620 | train loss: 0.00870727188885212 | train acc 0.7807407379150391
step: 630 | train loss: 0.008665986359119415 | train acc 0.7489933371543884
step: 640 | train loss: 0.008670482784509659 | train acc 0.7750000357627869
step: 650 | train loss: 0.009348833002150059 | train acc 0.7677053809165955
step: 660 | train loss: 0.008524397388100624 | train acc 0.767346978187561
step: 670 | train loss: 0.009265171363949776 | train acc 0.7517337203025818
step: 680 | train loss: 0.007923014461994171 | train acc 0.7648613452911377
step: 690 | train loss: 0.008473551832139492 | train acc 0.7630854249000549
step: 700 | train loss: 0.008701343089342117 | train acc 0.7742857336997986
step: 710 | train loss: 0.010587606579065323 | train acc 0.7484756112098694
step: 720 | train loss: 0.008802034892141819 | train acc 0.7538036108016968
step: 730 | train loss: 0.008335371501743793 | train acc 0.7758389711380005
step: 740 | train loss: 0.009480380453169346 | train acc 0.77028888463974
step: 750 | train loss: 0.008948259055614471 | train acc 0.7795918583869934
step: 760 | train loss: 0.008284949697554111 | train acc 0.779891312122345
step: 770 | train loss: 0.009110638871788979 | train acc 0.779940128326416
step: 780 | train loss: 0.008933856151998043 | train acc 0.7418032288551331
step: 790 | train loss: 0.008339791558682919 | train acc 0.7734807133674622
step: 800 | train loss: 0.008264890871942043 | train acc 0.765753448009491
step: 810 | train loss: 0.008265571668744087 | train acc 0.7782369256019592
step: 820 | train loss: 0.0082617262378335 | train acc 0.7736351490020752
step: 830 | train loss: 0.008797434158623219 | train acc 0.7482805848121643
step: 840 | train loss: 0.00920301117002964 | train acc 0.769456684589386
step: 850 | train loss: 0.009482906199991703 | train acc 0.7235449552536011
step: 860 | train loss: 0.008868672885000706 | train acc 0.7513812184333801
step: 870 | train loss: 0.008825672790408134 | train acc 0.7777777910232544
step: 880 | train loss: 0.00784987397491932 | train acc 0.7861035466194153
step: 890 | train loss: 0.00785108096897602 | train acc 0.7791164517402649
step: 900 | train loss: 0.010561949573457241 | train acc 0.7328125238418579
step: 910 | train loss: 0.006564173847436905 | train acc 0.9838709235191345
step: 920 | train loss: 0.0032756254076957703 | train acc 0.9469388127326965
step: 930 | train loss: 0.003985228016972542 | train acc 0.9070735573768616
step: 940 | train loss: 0.002638334408402443 | train acc 0.9344692230224609
step: 950 | train loss: 0.0024192482233047485 | train acc 0.9523809552192688
step: 960 | train loss: 0.003073042258620262 | train acc 0.9232839941978455
step: 970 | train loss: 0.002286212984472513 | train acc 0.9456067085266113
step: 980 | train loss: 0.002613708144053817 | train acc 0.9393019676208496
step: 990 | train loss: 0.0025857710279524326 | train acc 0.9364864826202393
step: 1000 | train loss: 0.0027213660068809986 | train acc 0.9359999895095825
step: 1010 | train loss: 0.0017591709038242698 | train acc 0.9629120826721191
step: 1020 | train loss: 0.0017535797087475657 | train acc 0.9586207270622253
step: 1030 | train loss: 0.0020851485896855593 | train acc 0.9474412202835083
step: 1040 | train loss: 0.0026093577034771442 | train acc 0.9368575215339661
step: 1050 | train loss: 0.002582178683951497 | train acc 0.9466292262077332
step: 1060 | train loss: 0.0023688958026468754 | train acc 0.941504180431366
step: 1070 | train loss: 0.0024159536696970463 | train acc 0.9384405016899109
step: 1080 | train loss: 0.0020957300439476967 | train acc 0.9420689940452576
step: 1090 | train loss: 0.002158117014914751 | train acc 0.9477911591529846
step: 1100 | train loss: 0.0016008035745471716 | train acc 0.9639769196510315
step: 1110 | train loss: 0.0022757542319595814 | train acc 0.9476743936538696
step: 1120 | train loss: 0.0018843634752556682 | train acc 0.9531034827232361
step: 1130 | train loss: 0.002320778090506792 | train acc 0.9297297596931458
step: 1140 | train loss: 0.002418224001303315 | train acc 0.9424083828926086
step: 1150 | train loss: 0.0020776132587343454 | train acc 0.9448276162147522
step: 1160 | train loss: 0.0019249690230935812 | train acc 0.9609484076499939
step: 1170 | train loss: 0.002129380591213703 | train acc 0.9626168012619019
step: 1180 | train loss: 0.0025548934936523438 | train acc 0.9377593398094177
step: 1190 | train loss: 0.0017577707767486572 | train acc 0.9562243819236755
step: 1200 | train loss: 0.0023671158123761415 | train acc 0.9356164336204529
step: 1210 | train loss: 0.0015152784762904048 | train acc 0.9575596451759338
step: 1220 | train loss: 0.0020608422346413136 | train acc 0.944223165512085
step: 1230 | train loss: 0.002818097360432148 | train acc 0.9519230723381042
step: 1240 | train loss: 0.0030502178706228733 | train acc 0.9250680804252625
step: 1250 | train loss: 0.002522723749279976 | train acc 0.9430894255638123
step: 1260 | train loss: 0.0017407985869795084 | train acc 0.9613333344459534
step: 1270 | train loss: 0.0019079321064054966 | train acc 0.9598930478096008
step: 1280 | train loss: 0.0023798393085598946 | train acc 0.9495224952697754
step: 1290 | train loss: 0.0022110501304268837 | train acc 0.9447514414787292
step: 1300 | train loss: 0.0030844477005302906 | train acc 0.9181817770004272
step: 1310 | train loss: 0.0031291714403778315 | train acc 0.9175824522972107
step: 1320 | train loss: 0.002459658542647958 | train acc 0.9505494832992554
step: 1330 | train loss: 0.0020253632683306932 | train acc 0.9451302886009216
step: 1340 | train loss: 0.0018890267238020897 | train acc 0.942577064037323
step: 1350 | train loss: 0.001439109561033547 | train acc 0.9599466323852539
step: 1360 | train loss: 0.0025825665798038244 | train acc 0.9383458495140076
step: 1370 | train loss: 0.002130944048985839 | train acc 0.9469388127326965
step: 1380 | train loss: 0.002998782554641366 | train acc 0.9298486709594727
step: 1390 | train loss: 0.0018675628816708922 | train acc 0.9522472023963928
step: 1400 | train loss: 0.0021640020422637463 | train acc 0.9481582641601562
step: 1410 | train loss: 0.0022044165525585413 | train acc 0.9378453493118286
step: 1420 | train loss: 0.0022159330546855927 | train acc 0.9460227489471436
step: 1430 | train loss: 0.002532295184209943 | train acc 0.9481481313705444
step: 1440 | train loss: 0.0029931741300970316 | train acc 0.930232584476471
step: 1450 | train loss: 0.002793701598420739 | train acc 0.9336941242218018
step: 1460 | train loss: 0.0023940028622746468 | train acc 0.9435483813285828
step: 1470 | train loss: 0.0020126933231949806 | train acc 0.9511855244636536
step: 1480 | train loss: 0.0015039221616461873 | train acc 0.9582753777503967
step: 1490 | train loss: 0.002592449774965644 | train acc 0.940030038356781
step: 1500 | train loss: 0.0021102535538375378 | train acc 0.9428969025611877
step: 1510 | train loss: 0.0024071799125522375 | train acc 0.9410958886146545
step: 1520 | train loss: 0.001662218477576971 | train acc 0.960544228553772
step: 1530 | train loss: 0.0019450928084552288 | train acc 0.9582772254943848
step: 1540 | train loss: 0.002182421274483204 | train acc 0.9418132305145264
step: 1550 | train loss: 0.0018603068310767412 | train acc 0.9581939578056335
step: 1560 | train loss: 0.0023534735664725304 | train acc 0.9368998408317566
step: 1570 | train loss: 0.002301987959071994 | train acc 0.9459084868431091
step: 1580 | train loss: 0.0016512034926563501 | train acc 0.9531459212303162
step: 1590 | train loss: 0.0032543851993978024 | train acc 0.9246031641960144
step: 1600 | train loss: 0.0016064868541434407 | train acc 0.9677852988243103
step: 1610 | train loss: 0.0017450959421694279 | train acc 0.9531680345535278
step: 1620 | train loss: 0.0026275275740772486 | train acc 0.9487179517745972
step: 1630 | train loss: 0.0014400114305317402 | train acc 0.9687055349349976
step: 1640 | train loss: 0.0023518959060311317 | train acc 0.9353932738304138
step: 1650 | train loss: 0.0019374978728592396 | train acc 0.9527027010917664
step: 1660 | train loss: 0.0020891674794256687 | train acc 0.9597222208976746
step: 1670 | train loss: 0.0021981969475746155 | train acc 0.9495224952697754
step: 1680 | train loss: 0.0026798092294484377 | train acc 0.9438700079917908
step: 1690 | train loss: 0.002565644681453705 | train acc 0.928473174571991
step: 1700 | train loss: 0.0024849025066941977 | train acc 0.9435262084007263
step: 1710 | train loss: 0.0022545852698385715 | train acc 0.9468664526939392
step: 1720 | train loss: 0.002898074220865965 | train acc 0.944444477558136
step: 1730 | train loss: 0.0021046672482043505 | train acc 0.9465020298957825
step: 1740 | train loss: 0.001999214058741927 | train acc 0.9540889859199524
step: 1750 | train loss: 0.0021046041510999203 | train acc 0.9492957592010498
step: 1760 | train loss: 0.0016193906776607037 | train acc 0.9566395878791809
step: 1770 | train loss: 0.002322031883522868 | train acc 0.9513888955116272
step: 1780 | train loss: 0.0021755751222372055 | train acc 0.948106586933136
step: 1790 | train loss: 0.0021235940512269735 | train acc 0.9478873014450073
step: 1800 | train loss: 0.0019413018599152565 | train acc 0.958865225315094
step: 1810 | train loss: 0.001932548126205802 | train acc 0.9514979124069214
***** Running evaluation *****
  Batch size = 8
***** Valid Eval results *****
  global_step = 1816
  valid_eval_accuracy = 0.7796870992562196
  valid_eval_loss = 0.7678305360616422
