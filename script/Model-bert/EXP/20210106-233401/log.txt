device cuda n_gpu 1 distributed training False
***** Running training *****
  Batch size = 4
  Num steps = 1816
step: 10 | train loss: 0.01493116095662117 | train acc 0.6583210825920105
step: 20 | train loss: 0.01285853236913681 | train acc 0.6753069758415222
step: 30 | train loss: 0.01178646832704544 | train acc 0.6927297711372375
step: 40 | train loss: 0.010747097432613373 | train acc 0.7207702994346619
step: 50 | train loss: 0.010005488991737366 | train acc 0.727148711681366
step: 60 | train loss: 0.01115864422172308 | train acc 0.7071823477745056
step: 70 | train loss: 0.012005670927464962 | train acc 0.6933534741401672
step: 80 | train loss: 0.010758409276604652 | train acc 0.7243767380714417
step: 90 | train loss: 0.010407405905425549 | train acc 0.7028112411499023
step: 100 | train loss: 0.010404789820313454 | train acc 0.7074176073074341
step: 110 | train loss: 0.010079288855195045 | train acc 0.7455048561096191
step: 120 | train loss: 0.00885376613587141 | train acc 0.7706044316291809
step: 130 | train loss: 0.013263574801385403 | train acc 0.6802413463592529
step: 140 | train loss: 0.010376800782978535 | train acc 0.7062069177627563
step: 150 | train loss: 0.01245067548006773 | train acc 0.666208803653717
step: 160 | train loss: 0.010912577621638775 | train acc 0.6917712688446045
step: 170 | train loss: 0.01048679742962122 | train acc 0.6908602118492126
step: 180 | train loss: 0.009654873982071877 | train acc 0.7418032288551331
step: 190 | train loss: 0.011965468525886536 | train acc 0.6712707281112671
step: 200 | train loss: 0.011788778007030487 | train acc 0.6791366934776306
step: 210 | train loss: 0.010414062067866325 | train acc 0.7057182788848877
step: 220 | train loss: 0.00932847335934639 | train acc 0.7426666617393494
step: 230 | train loss: 0.009466825053095818 | train acc 0.7420690059661865
step: 240 | train loss: 0.01025541964918375 | train acc 0.7034013867378235
step: 250 | train loss: 0.009764705784618855 | train acc 0.7105624079704285
step: 260 | train loss: 0.012467650696635246 | train acc 0.6971935033798218
step: 270 | train loss: 0.010984750464558601 | train acc 0.6964769959449768
step: 280 | train loss: 0.010143458843231201 | train acc 0.7112010717391968
step: 290 | train loss: 0.010585688054561615 | train acc 0.7056337594985962
step: 300 | train loss: 0.010708237998187542 | train acc 0.698499321937561
step: 310 | train loss: 0.009783599525690079 | train acc 0.7423822283744812
step: 320 | train loss: 0.010428009554743767 | train acc 0.7059659361839294
step: 330 | train loss: 0.010338990017771721 | train acc 0.7300000190734863
step: 340 | train loss: 0.009406093508005142 | train acc 0.744911789894104
step: 350 | train loss: 0.008725407533347607 | train acc 0.7635605335235596
step: 360 | train loss: 0.008871614933013916 | train acc 0.7517241835594177
step: 370 | train loss: 0.010471249930560589 | train acc 0.7020134329795837
step: 380 | train loss: 0.009487006813287735 | train acc 0.7292817831039429
step: 390 | train loss: 0.011320721358060837 | train acc 0.7372742295265198
step: 400 | train loss: 0.010096674785017967 | train acc 0.7493113279342651
step: 410 | train loss: 0.008905849419534206 | train acc 0.7601115703582764
step: 420 | train loss: 0.00869052391499281 | train acc 0.7763713002204895
step: 430 | train loss: 0.008945978246629238 | train acc 0.7732394337654114
step: 440 | train loss: 0.00986337661743164 | train acc 0.7165775299072266
step: 450 | train loss: 0.008880749344825745 | train acc 0.782071053981781
step: 460 | train loss: 0.009536316618323326 | train acc 0.747922420501709
step: 470 | train loss: 0.009609652683138847 | train acc 0.720708429813385
step: 480 | train loss: 0.008336098864674568 | train acc 0.7553476095199585
step: 490 | train loss: 0.00977579690515995 | train acc 0.727150559425354
step: 500 | train loss: 0.00843887124210596 | train acc 0.7641379833221436
step: 510 | train loss: 0.008458654396235943 | train acc 0.7822932004928589
step: 520 | train loss: 0.010540066286921501 | train acc 0.7441860437393188
step: 530 | train loss: 0.009281034581363201 | train acc 0.7441217303276062
step: 540 | train loss: 0.009745701216161251 | train acc 0.7420250177383423
step: 550 | train loss: 0.0095876669511199 | train acc 0.7318741679191589
step: 560 | train loss: 0.01015587616711855 | train acc 0.7203390002250671
step: 570 | train loss: 0.008589834906160831 | train acc 0.7677852511405945
step: 580 | train loss: 0.00931673962622881 | train acc 0.7636632323265076
step: 590 | train loss: 0.009395027533173561 | train acc 0.7337931394577026
step: 600 | train loss: 0.008821696974337101 | train acc 0.7520661354064941
step: 610 | train loss: 0.008959360420703888 | train acc 0.73777174949646
step: 620 | train loss: 0.008829105645418167 | train acc 0.7574931979179382
step: 630 | train loss: 0.007702490780502558 | train acc 0.792276918888092
step: 640 | train loss: 0.010111082345247269 | train acc 0.7412587404251099
step: 650 | train loss: 0.01023002527654171 | train acc 0.7296137809753418
step: 660 | train loss: 0.00854340847581625 | train acc 0.7823033928871155
step: 670 | train loss: 0.008037365041673183 | train acc 0.7928177118301392
step: 680 | train loss: 0.009055922739207745 | train acc 0.75
step: 690 | train loss: 0.009602968581020832 | train acc 0.7229729890823364
step: 700 | train loss: 0.00785186979919672 | train acc 0.7969613671302795
step: 710 | train loss: 0.00954088568687439 | train acc 0.7901991009712219
step: 720 | train loss: 0.008603760972619057 | train acc 0.774324357509613
step: 730 | train loss: 0.009086808189749718 | train acc 0.7533692717552185
step: 740 | train loss: 0.008717792108654976 | train acc 0.767123281955719
step: 750 | train loss: 0.008896874263882637 | train acc 0.7530040144920349
step: 760 | train loss: 0.008964507840573788 | train acc 0.7560646533966064
step: 770 | train loss: 0.011375621892511845 | train acc 0.7684887051582336
step: 780 | train loss: 0.010341854766011238 | train acc 0.7311385273933411
step: 790 | train loss: 0.009521296247839928 | train acc 0.7685950398445129
step: 800 | train loss: 0.007765977643430233 | train acc 0.7807847857475281
step: 810 | train loss: 0.009206182323396206 | train acc 0.7426666617393494
step: 820 | train loss: 0.008436527103185654 | train acc 0.77173912525177
step: 830 | train loss: 0.008419149555265903 | train acc 0.7531380653381348
step: 840 | train loss: 0.009179972112178802 | train acc 0.7449856996536255
step: 850 | train loss: 0.0097403135150671 | train acc 0.7541436553001404
step: 860 | train loss: 0.007992051541805267 | train acc 0.770691990852356
step: 870 | train loss: 0.009247311390936375 | train acc 0.7527027130126953
step: 880 | train loss: 0.008585099130868912 | train acc 0.7700680494308472
step: 890 | train loss: 0.007658668793737888 | train acc 0.7827868461608887
step: 900 | train loss: 0.010274291969835758 | train acc 0.7460814714431763
step: 910 | train loss: 0.0038487848360091448 | train acc 0.9651162624359131
step: 920 | train loss: 0.0037620405200868845 | train acc 0.928475022315979
step: 930 | train loss: 0.0029131784103810787 | train acc 0.9398396015167236
step: 940 | train loss: 0.002739703981205821 | train acc 0.9347826242446899
step: 950 | train loss: 0.0021080963779240847 | train acc 0.9601648449897766
step: 960 | train loss: 0.0024274694733321667 | train acc 0.9484396576881409
step: 970 | train loss: 0.0022624211851507425 | train acc 0.9553695917129517
step: 980 | train loss: 0.0028197283390909433 | train acc 0.9380281567573547
step: 990 | train loss: 0.0021609056275337934 | train acc 0.9464285969734192
step: 1000 | train loss: 0.002585621317848563 | train acc 0.9335142374038696
step: 1010 | train loss: 0.00210573454387486 | train acc 0.9468531608581543
step: 1020 | train loss: 0.00237070769071579 | train acc 0.9403606653213501
step: 1030 | train loss: 0.002347594825550914 | train acc 0.9428571462631226
step: 1040 | train loss: 0.0022047737147659063 | train acc 0.94797682762146
step: 1050 | train loss: 0.0018235872266814113 | train acc 0.958776593208313
step: 1060 | train loss: 0.002373565686866641 | train acc 0.9422819018363953
step: 1070 | train loss: 0.0019075912423431873 | train acc 0.95576411485672
step: 1080 | train loss: 0.001973670208826661 | train acc 0.9499322772026062
step: 1090 | train loss: 0.0025658346712589264 | train acc 0.9401360750198364
step: 1100 | train loss: 0.001969072036445141 | train acc 0.9615384340286255
step: 1110 | train loss: 0.0019206692231819034 | train acc 0.9590643644332886
step: 1120 | train loss: 0.002517357235774398 | train acc 0.9394773244857788
step: 1130 | train loss: 0.0029571233317255974 | train acc 0.9335219264030457
step: 1140 | train loss: 0.002065862063318491 | train acc 0.9482288956642151
step: 1150 | train loss: 0.0021818294189870358 | train acc 0.9440745115280151
step: 1160 | train loss: 0.0018872902728617191 | train acc 0.9554054141044617
step: 1170 | train loss: 0.002058021491393447 | train acc 0.9598070383071899
step: 1180 | train loss: 0.002253324491903186 | train acc 0.9385245442390442
step: 1190 | train loss: 0.0031799091957509518 | train acc 0.9267299771308899
step: 1200 | train loss: 0.0025001075118780136 | train acc 0.927496612071991
step: 1210 | train loss: 0.002576557919383049 | train acc 0.9339498281478882
step: 1220 | train loss: 0.002079334110021591 | train acc 0.9594045877456665
step: 1230 | train loss: 0.001584494486451149 | train acc 0.964963436126709
step: 1240 | train loss: 0.002513174433261156 | train acc 0.9420084953308105
step: 1250 | train loss: 0.0020409761928021908 | train acc 0.9581589698791504
step: 1260 | train loss: 0.002977906260639429 | train acc 0.9403794407844543
step: 1270 | train loss: 0.0019252477213740349 | train acc 0.94972825050354
step: 1280 | train loss: 0.0022406424395740032 | train acc 0.9471544623374939
step: 1290 | train loss: 0.0031051896512508392 | train acc 0.9405797719955444
step: 1300 | train loss: 0.0026548763271421194 | train acc 0.9451219439506531
step: 1310 | train loss: 0.0022630970925092697 | train acc 0.942307710647583
step: 1320 | train loss: 0.0030675502493977547 | train acc 0.9381868243217468
step: 1330 | train loss: 0.0017911531031131744 | train acc 0.9554054141044617
step: 1340 | train loss: 0.0026244891341775656 | train acc 0.9385474324226379
step: 1350 | train loss: 0.0020339631009846926 | train acc 0.9431818723678589
step: 1360 | train loss: 0.002861945889890194 | train acc 0.9440994262695312
step: 1370 | train loss: 0.0018103575566783547 | train acc 0.9580419659614563
step: 1380 | train loss: 0.003413389204069972 | train acc 0.9237170815467834
step: 1390 | train loss: 0.00268348166719079 | train acc 0.924119234085083
step: 1400 | train loss: 0.0027581453323364258 | train acc 0.9402174353599548
step: 1410 | train loss: 0.002128326566889882 | train acc 0.9402174353599548
step: 1420 | train loss: 0.002478100825101137 | train acc 0.9452449083328247
step: 1430 | train loss: 0.002928372472524643 | train acc 0.9352290630340576
step: 1440 | train loss: 0.0024607402738183737 | train acc 0.9491525292396545
step: 1450 | train loss: 0.0026777759194374084 | train acc 0.9453015327453613
step: 1460 | train loss: 0.0021566657815128565 | train acc 0.9450549483299255
step: 1470 | train loss: 0.0028651838656514883 | train acc 0.941678524017334
step: 1480 | train loss: 0.002737093949690461 | train acc 0.9272976517677307
step: 1490 | train loss: 0.0022504478693008423 | train acc 0.9550898671150208
step: 1500 | train loss: 0.0033036486711353064 | train acc 0.9226666688919067
step: 1510 | train loss: 0.0019780034199357033 | train acc 0.9496598839759827
step: 1520 | train loss: 0.002321455394849181 | train acc 0.9455307126045227
step: 1530 | train loss: 0.0019025482470169663 | train acc 0.9572192430496216
step: 1540 | train loss: 0.0017393991583958268 | train acc 0.9562333822250366
step: 1550 | train loss: 0.0018935714615508914 | train acc 0.9591836929321289
step: 1560 | train loss: 0.001991651952266693 | train acc 0.9540389776229858
step: 1570 | train loss: 0.0025480245240032673 | train acc 0.9367977380752563
step: 1580 | train loss: 0.0022426065988838673 | train acc 0.9445983171463013
step: 1590 | train loss: 0.002121095545589924 | train acc 0.9426573514938354
step: 1600 | train loss: 0.001967683434486389 | train acc 0.94701087474823
step: 1610 | train loss: 0.002183401957154274 | train acc 0.9486803412437439
step: 1620 | train loss: 0.001986890332773328 | train acc 0.9439527988433838
step: 1630 | train loss: 0.0017760525224730372 | train acc 0.9582772254943848
step: 1640 | train loss: 0.001675478764809668 | train acc 0.9612299799919128
step: 1650 | train loss: 0.0015651360154151917 | train acc 0.9695290327072144
step: 1660 | train loss: 0.0015589907998219132 | train acc 0.9574467539787292
step: 1670 | train loss: 0.0022615892812609673 | train acc 0.9398085474967957
step: 1680 | train loss: 0.004281059373170137 | train acc 0.9428571462631226
step: 1690 | train loss: 0.004986743908375502 | train acc 0.8951724171638489
step: 1700 | train loss: 0.002193230902776122 | train acc 0.949438214302063
step: 1710 | train loss: 0.0019694375805556774 | train acc 0.9525101780891418
step: 1720 | train loss: 0.0018236933974549174 | train acc 0.9511533379554749
step: 1730 | train loss: 0.0015564466593787074 | train acc 0.9651007056236267
step: 1740 | train loss: 0.0017405572580173612 | train acc 0.95652174949646
step: 1750 | train loss: 0.0028834461700171232 | train acc 0.934451162815094
step: 1760 | train loss: 0.002175932750105858 | train acc 0.9501384496688843
step: 1770 | train loss: 0.002154030604287982 | train acc 0.950863242149353
step: 1780 | train loss: 0.0026915862690657377 | train acc 0.9436038732528687
step: 1790 | train loss: 0.002061522798612714 | train acc 0.9459459781646729
step: 1800 | train loss: 0.0017580314306542277 | train acc 0.952766478061676
step: 1810 | train loss: 0.002213088795542717 | train acc 0.9589235186576843
***** Running evaluation *****
  Batch size = 8
***** Valid Eval results *****
  global_step = 1816
  valid_eval_accuracy = 0.7831495255193639
  valid_eval_loss = 0.7451042994564655
