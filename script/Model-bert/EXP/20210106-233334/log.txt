device cuda n_gpu 1 distributed training False
***** Running training *****
  Batch size = 4
  Num steps = 1815
step: 10 | train loss: 0.015201370231807232 | train acc 0.665217399597168
step: 20 | train loss: 0.011233818717300892 | train acc 0.7166666984558105
step: 30 | train loss: 0.01177853811532259 | train acc 0.68733149766922
step: 40 | train loss: 0.010679316706955433 | train acc 0.6880108714103699
step: 50 | train loss: 0.00999643187969923 | train acc 0.7375690937042236
step: 60 | train loss: 0.01135388482362032 | train acc 0.6894665360450745
step: 70 | train loss: 0.011860172264277935 | train acc 0.6943573355674744
step: 80 | train loss: 0.011086108162999153 | train acc 0.7142857313156128
step: 90 | train loss: 0.010777484625577927 | train acc 0.6892808675765991
step: 100 | train loss: 0.010268823243677616 | train acc 0.7103825211524963
step: 110 | train loss: 0.009879138320684433 | train acc 0.7298049926757812
step: 120 | train loss: 0.010133035480976105 | train acc 0.7023809552192688
step: 130 | train loss: 0.011673378758132458 | train acc 0.7147335410118103
step: 140 | train loss: 0.010982165113091469 | train acc 0.7039105892181396
step: 150 | train loss: 0.009919444099068642 | train acc 0.7327001690864563
step: 160 | train loss: 0.010808657854795456 | train acc 0.7093185186386108
step: 170 | train loss: 0.010050054639577866 | train acc 0.6924101114273071
step: 180 | train loss: 0.010712115094065666 | train acc 0.6988950371742249
step: 190 | train loss: 0.010382073931396008 | train acc 0.7261236310005188
step: 200 | train loss: 0.010546057485044003 | train acc 0.7108262181282043
step: 210 | train loss: 0.010223985649645329 | train acc 0.7158620953559875
step: 220 | train loss: 0.009691040962934494 | train acc 0.7354925274848938
step: 230 | train loss: 0.01046609878540039 | train acc 0.6993007063865662
step: 240 | train loss: 0.010400396771728992 | train acc 0.7100840210914612
step: 250 | train loss: 0.011112759821116924 | train acc 0.6916890144348145
step: 260 | train loss: 0.011693078093230724 | train acc 0.7289433479309082
step: 270 | train loss: 0.009667985141277313 | train acc 0.717391312122345
step: 280 | train loss: 0.009013006463646889 | train acc 0.744966447353363
step: 290 | train loss: 0.010792067274451256 | train acc 0.7032520174980164
step: 300 | train loss: 0.010713200084865093 | train acc 0.6900269389152527
step: 310 | train loss: 0.00911676324903965 | train acc 0.7307692766189575
step: 320 | train loss: 0.009162254631519318 | train acc 0.7619718313217163
step: 330 | train loss: 0.01193841639906168 | train acc 0.6998491883277893
step: 340 | train loss: 0.010545083321630955 | train acc 0.6928281188011169
step: 350 | train loss: 0.009682134725153446 | train acc 0.7112010717391968
step: 360 | train loss: 0.00983365997672081 | train acc 0.7411924004554749
step: 370 | train loss: 0.009319441393017769 | train acc 0.7621622085571289
step: 380 | train loss: 0.009196964092552662 | train acc 0.7542135119438171
step: 390 | train loss: 0.012632617726922035 | train acc 0.7001647353172302
step: 400 | train loss: 0.009961261413991451 | train acc 0.7170596718788147
step: 410 | train loss: 0.008502268232405186 | train acc 0.7852257490158081
step: 420 | train loss: 0.00976586900651455 | train acc 0.7205882668495178
step: 430 | train loss: 0.009525835514068604 | train acc 0.7380318641662598
step: 440 | train loss: 0.009416998364031315 | train acc 0.7480105757713318
step: 450 | train loss: 0.010337539948523045 | train acc 0.7441860437393188
step: 460 | train loss: 0.010996823199093342 | train acc 0.7219858169555664
step: 470 | train loss: 0.009141426533460617 | train acc 0.7337931394577026
step: 480 | train loss: 0.009012977592647076 | train acc 0.7542135119438171
step: 490 | train loss: 0.008035206235945225 | train acc 0.7786774635314941
step: 500 | train loss: 0.008757654577493668 | train acc 0.7734057307243347
step: 510 | train loss: 0.009868554770946503 | train acc 0.739130437374115
step: 520 | train loss: 0.009505439549684525 | train acc 0.7496229410171509
step: 530 | train loss: 0.010166413150727749 | train acc 0.7142857313156128
step: 540 | train loss: 0.008122563362121582 | train acc 0.7786774635314941
step: 550 | train loss: 0.008490695618093014 | train acc 0.7794730067253113
step: 560 | train loss: 0.00909348577260971 | train acc 0.7533875703811646
step: 570 | train loss: 0.008023618720471859 | train acc 0.7726638913154602
step: 580 | train loss: 0.011067601852118969 | train acc 0.7284671068191528
step: 590 | train loss: 0.008224911987781525 | train acc 0.7782257795333862
step: 600 | train loss: 0.009166053496301174 | train acc 0.7493074536323547
step: 610 | train loss: 0.007881492376327515 | train acc 0.7935656905174255
step: 620 | train loss: 0.00853502843528986 | train acc 0.7730398774147034
step: 630 | train loss: 0.008314167149364948 | train acc 0.767978310585022
step: 640 | train loss: 0.008422278799116611 | train acc 0.7617135047912598
step: 650 | train loss: 0.00938474852591753 | train acc 0.7378223538398743
step: 660 | train loss: 0.008325997740030289 | train acc 0.7815468311309814
step: 670 | train loss: 0.008508600294589996 | train acc 0.7863013744354248
step: 680 | train loss: 0.008419657126069069 | train acc 0.7693333029747009
step: 690 | train loss: 0.00960735883563757 | train acc 0.7506738305091858
step: 700 | train loss: 0.008306922391057014 | train acc 0.7706044316291809
step: 710 | train loss: 0.009733393788337708 | train acc 0.7581395506858826
step: 720 | train loss: 0.010114670731127262 | train acc 0.7279005646705627
step: 730 | train loss: 0.010034053586423397 | train acc 0.726402223110199
step: 740 | train loss: 0.008736194111406803 | train acc 0.7755960822105408
step: 750 | train loss: 0.008797773160040379 | train acc 0.7777777910232544
step: 760 | train loss: 0.009070445783436298 | train acc 0.7586685419082642
step: 770 | train loss: 0.00843615923076868 | train acc 0.7793696522712708
step: 780 | train loss: 0.008905592374503613 | train acc 0.757241427898407
step: 790 | train loss: 0.009866858832538128 | train acc 0.7609289288520813
step: 800 | train loss: 0.00910007394850254 | train acc 0.7538036108016968
step: 810 | train loss: 0.008651561103761196 | train acc 0.7534435391426086
step: 820 | train loss: 0.007670428138226271 | train acc 0.7871760129928589
step: 830 | train loss: 0.008469847962260246 | train acc 0.7691237926483154
step: 840 | train loss: 0.009018640965223312 | train acc 0.7681818008422852
step: 850 | train loss: 0.009859774261713028 | train acc 0.7496580481529236
step: 860 | train loss: 0.008090445771813393 | train acc 0.7658142447471619
step: 870 | train loss: 0.008320938795804977 | train acc 0.763157844543457
step: 880 | train loss: 0.009826810099184513 | train acc 0.7412399053573608
step: 890 | train loss: 0.009398083202540874 | train acc 0.7612994313240051
step: 900 | train loss: 0.008889678865671158 | train acc 0.7700296640396118
step: 910 | train loss: 0.0033154787961393595 | train acc 0.9319727420806885
step: 920 | train loss: 0.003421199042350054 | train acc 0.936813235282898
step: 930 | train loss: 0.002731988439336419 | train acc 0.9566395878791809
step: 940 | train loss: 0.002460736781358719 | train acc 0.9546703696250916
step: 950 | train loss: 0.0026341609191149473 | train acc 0.9468084573745728
step: 960 | train loss: 0.003125622868537903 | train acc 0.9312668442726135
step: 970 | train loss: 0.002196972258388996 | train acc 0.9514979124069214
step: 980 | train loss: 0.0021792214829474688 | train acc 0.9514867067337036
step: 990 | train loss: 0.002979977522045374 | train acc 0.9356164336204529
step: 1000 | train loss: 0.002488136989995837 | train acc 0.9412568211555481
step: 1010 | train loss: 0.002430744469165802 | train acc 0.9464285969734192
step: 1020 | train loss: 0.0031937607564032078 | train acc 0.9202200770378113
step: 1030 | train loss: 0.002156616887077689 | train acc 0.9528936743736267
step: 1040 | train loss: 0.0024335593916475773 | train acc 0.9492868781089783
step: 1050 | train loss: 0.0024115941487252712 | train acc 0.9402984976768494
step: 1060 | train loss: 0.0030509093776345253 | train acc 0.9273504018783569
step: 1070 | train loss: 0.002548886463046074 | train acc 0.9282869100570679
step: 1080 | train loss: 0.0018899485003203154 | train acc 0.949400782585144
step: 1090 | train loss: 0.0017643492901697755 | train acc 0.9513797163963318
step: 1100 | train loss: 0.002193886088207364 | train acc 0.9539568424224854
step: 1110 | train loss: 0.003656763583421707 | train acc 0.9191176891326904
step: 1120 | train loss: 0.0030628989916294813 | train acc 0.919288694858551
step: 1130 | train loss: 0.0024830447509884834 | train acc 0.939393937587738
step: 1140 | train loss: 0.001814431045204401 | train acc 0.9637096524238586
step: 1150 | train loss: 0.0017369065899401903 | train acc 0.9623430967330933
step: 1160 | train loss: 0.001994787249714136 | train acc 0.952380895614624
step: 1170 | train loss: 0.0020222263410687447 | train acc 0.9567099809646606
step: 1180 | train loss: 0.0020206766203045845 | train acc 0.9500000476837158
step: 1190 | train loss: 0.0023943299893289804 | train acc 0.936742901802063
step: 1200 | train loss: 0.001841578516177833 | train acc 0.9581005573272705
step: 1210 | train loss: 0.0017495887586846948 | train acc 0.9585635662078857
step: 1220 | train loss: 0.002432903740555048 | train acc 0.9356568455696106
step: 1230 | train loss: 0.002406591782346368 | train acc 0.9397059082984924
step: 1240 | train loss: 0.00232853926718235 | train acc 0.9434483051300049
step: 1250 | train loss: 0.0024672134313732386 | train acc 0.9392712116241455
step: 1260 | train loss: 0.002031678566709161 | train acc 0.9568150639533997
step: 1270 | train loss: 0.002075138036161661 | train acc 0.94972825050354
step: 1280 | train loss: 0.002051771152764559 | train acc 0.9481382966041565
step: 1290 | train loss: 0.00215964880771935 | train acc 0.9395217895507812
step: 1300 | train loss: 0.0033441439736634493 | train acc 0.9291784763336182
step: 1310 | train loss: 0.0018874985398724675 | train acc 0.9568845629692078
step: 1320 | train loss: 0.0018071939703077078 | train acc 0.9544845819473267
step: 1330 | train loss: 0.0024242193903774023 | train acc 0.9460916519165039
step: 1340 | train loss: 0.0025668831076472998 | train acc 0.9478737711906433
step: 1350 | train loss: 0.0018350319005548954 | train acc 0.9538462162017822
step: 1360 | train loss: 0.0019402586622163653 | train acc 0.9525926113128662
step: 1370 | train loss: 0.0017131445929408073 | train acc 0.9585062265396118
step: 1380 | train loss: 0.002167251193895936 | train acc 0.944212019443512
step: 1390 | train loss: 0.0020220051519572735 | train acc 0.9438356161117554
step: 1400 | train loss: 0.0017733462154865265 | train acc 0.9543010592460632
step: 1410 | train loss: 0.002132075373083353 | train acc 0.95546555519104
step: 1420 | train loss: 0.002189411548897624 | train acc 0.9524564743041992
step: 1430 | train loss: 0.0020637777633965015 | train acc 0.9550898671150208
step: 1440 | train loss: 0.0019209024030715227 | train acc 0.9608108401298523
step: 1450 | train loss: 0.0028859353624284267 | train acc 0.9410977363586426
step: 1460 | train loss: 0.0025225214194506407 | train acc 0.9391891956329346
step: 1470 | train loss: 0.002035364741459489 | train acc 0.9515905976295471
step: 1480 | train loss: 0.0021399070974439383 | train acc 0.9400278925895691
step: 1490 | train loss: 0.002581438049674034 | train acc 0.9463414549827576
step: 1500 | train loss: 0.0016632273327559233 | train acc 0.9642366170883179
step: 1510 | train loss: 0.0021839316468685865 | train acc 0.947945237159729
step: 1520 | train loss: 0.0027799936942756176 | train acc 0.9306122660636902
step: 1530 | train loss: 0.0019343942403793335 | train acc 0.9553072452545166
step: 1540 | train loss: 0.002457740716636181 | train acc 0.9439890384674072
step: 1550 | train loss: 0.0019774106331169605 | train acc 0.9528875350952148
step: 1560 | train loss: 0.0019280093256384134 | train acc 0.9478873014450073
step: 1570 | train loss: 0.002953195944428444 | train acc 0.9271254539489746
step: 1580 | train loss: 0.001563908881507814 | train acc 0.9593837857246399
step: 1590 | train loss: 0.0022935238666832447 | train acc 0.9466292262077332
step: 1600 | train loss: 0.0026260826271027327 | train acc 0.9359672665596008
step: 1610 | train loss: 0.0025555058382451534 | train acc 0.9442856907844543
step: 1620 | train loss: 0.0028457967564463615 | train acc 0.9341893792152405
step: 1630 | train loss: 0.0017297095619142056 | train acc 0.9608636498451233
step: 1640 | train loss: 0.0024741480592638254 | train acc 0.9577080607414246
step: 1650 | train loss: 0.0021721867378801107 | train acc 0.9506849646568298
step: 1660 | train loss: 0.0017290518153458834 | train acc 0.9582172632217407
step: 1670 | train loss: 0.0018317694775760174 | train acc 0.9552239179611206
step: 1680 | train loss: 0.0030811764299869537 | train acc 0.9332365989685059
step: 1690 | train loss: 0.0029042093083262444 | train acc 0.9293785095214844
step: 1700 | train loss: 0.0026854213792830706 | train acc 0.9338731169700623
step: 1710 | train loss: 0.0025628656148910522 | train acc 0.948106586933136
step: 1720 | train loss: 0.00137383583933115 | train acc 0.9673024415969849
step: 1730 | train loss: 0.0022172369062900543 | train acc 0.9395973682403564
step: 1740 | train loss: 0.0022720058914273977 | train acc 0.9389204978942871
step: 1750 | train loss: 0.0031599400099366903 | train acc 0.9252199530601501
step: 1760 | train loss: 0.002153325593098998 | train acc 0.9600551128387451
step: 1770 | train loss: 0.0019134936155751348 | train acc 0.9541160464286804
step: 1780 | train loss: 0.002254611114040017 | train acc 0.9388039112091064
step: 1790 | train loss: 0.002582908608019352 | train acc 0.9395604729652405
step: 1800 | train loss: 0.002128238556906581 | train acc 0.9537037014961243
step: 1810 | train loss: 0.0022318202536553144 | train acc 0.9507735371589661
***** Running evaluation *****
  Batch size = 8
***** Valid Eval results *****
  global_step = 1816
  valid_eval_accuracy = 0.7875096178507309
  valid_eval_loss = 0.7448147167177761
