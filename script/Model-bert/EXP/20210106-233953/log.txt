device cuda n_gpu 1 distributed training False
***** Running training *****
  Batch size = 4
  Num steps = 1812
step: 10 | train loss: 0.013238435611128807 | train acc 0.6775510311126709
step: 20 | train loss: 0.011572107672691345 | train acc 0.7068493366241455
step: 30 | train loss: 0.010438826866447926 | train acc 0.7355371713638306
step: 40 | train loss: 0.010177784599363804 | train acc 0.717391312122345
step: 50 | train loss: 0.009278054349124432 | train acc 0.7609618306159973
step: 60 | train loss: 0.010328666307032108 | train acc 0.7208333611488342
step: 70 | train loss: 0.012874370440840721 | train acc 0.6741741895675659
step: 80 | train loss: 0.01089728157967329 | train acc 0.6819406747817993
step: 90 | train loss: 0.01155970711261034 | train acc 0.716438353061676
step: 100 | train loss: 0.010218770243227482 | train acc 0.7176634073257446
step: 110 | train loss: 0.009060614742338657 | train acc 0.7649456858634949
step: 120 | train loss: 0.011556712910532951 | train acc 0.6894150376319885
step: 130 | train loss: 0.012718708254396915 | train acc 0.672386884689331
step: 140 | train loss: 0.011468685232102871 | train acc 0.6662107110023499
step: 150 | train loss: 0.01155412383377552 | train acc 0.6868829727172852
step: 160 | train loss: 0.010188656859099865 | train acc 0.7173333168029785
step: 170 | train loss: 0.01139318011701107 | train acc 0.6925170421600342
step: 180 | train loss: 0.012319567613303661 | train acc 0.6625173091888428
step: 190 | train loss: 0.010257752612233162 | train acc 0.7111716270446777
step: 200 | train loss: 0.012595577165484428 | train acc 0.6629921197891235
step: 210 | train loss: 0.01145540364086628 | train acc 0.7026647925376892
step: 220 | train loss: 0.01151690911501646 | train acc 0.6647966504096985
step: 230 | train loss: 0.010186419822275639 | train acc 0.6887685656547546
step: 240 | train loss: 0.010745128616690636 | train acc 0.6844261884689331
step: 250 | train loss: 0.009394284337759018 | train acc 0.72691810131073
step: 260 | train loss: 0.010513996705412865 | train acc 0.6984785795211792
step: 270 | train loss: 0.010969080962240696 | train acc 0.6811988949775696
step: 280 | train loss: 0.010774197056889534 | train acc 0.6847826242446899
step: 290 | train loss: 0.010916552506387234 | train acc 0.688022255897522
step: 300 | train loss: 0.011127318255603313 | train acc 0.6952908635139465
step: 310 | train loss: 0.010121198371052742 | train acc 0.7286713719367981
step: 320 | train loss: 0.0096872104331851 | train acc 0.7529239654541016
step: 330 | train loss: 0.010274074971675873 | train acc 0.7467249035835266
step: 340 | train loss: 0.00975769478827715 | train acc 0.7266576290130615
step: 350 | train loss: 0.009873631410300732 | train acc 0.7385444641113281
step: 360 | train loss: 0.009605768136680126 | train acc 0.7327001690864563
step: 370 | train loss: 0.009383129887282848 | train acc 0.721552848815918
step: 380 | train loss: 0.010602396912872791 | train acc 0.6912751793861389
step: 390 | train loss: 0.010211407206952572 | train acc 0.7313868403434753
step: 400 | train loss: 0.010493987239897251 | train acc 0.7094133496284485
step: 410 | train loss: 0.008992432616651058 | train acc 0.7546666860580444
step: 420 | train loss: 0.008973955176770687 | train acc 0.7449392676353455
step: 430 | train loss: 0.010598919354379177 | train acc 0.7248603105545044
step: 440 | train loss: 0.009663589298725128 | train acc 0.7430092692375183
step: 450 | train loss: 0.01141346711665392 | train acc 0.7218844890594482
step: 460 | train loss: 0.010400587692856789 | train acc 0.7392510771751404
step: 470 | train loss: 0.008835181593894958 | train acc 0.7523940205574036
step: 480 | train loss: 0.009645395912230015 | train acc 0.73097825050354
step: 490 | train loss: 0.009377622976899147 | train acc 0.7427785396575928
step: 500 | train loss: 0.009138193912804127 | train acc 0.7343957424163818
step: 510 | train loss: 0.009376103058457375 | train acc 0.7406877279281616
step: 520 | train loss: 0.00953004788607359 | train acc 0.7667185068130493
step: 530 | train loss: 0.009646245278418064 | train acc 0.7557471394538879
step: 540 | train loss: 0.00906774215400219 | train acc 0.7644683718681335
step: 550 | train loss: 0.01017894595861435 | train acc 0.7324930429458618
step: 560 | train loss: 0.009333059191703796 | train acc 0.7513812184333801
step: 570 | train loss: 0.00909846369177103 | train acc 0.7510373592376709
step: 580 | train loss: 0.01164163276553154 | train acc 0.7339593172073364
step: 590 | train loss: 0.009962009266018867 | train acc 0.7394365668296814
step: 600 | train loss: 0.008471345528960228 | train acc 0.7534059882164001
step: 610 | train loss: 0.008051693439483643 | train acc 0.7815468311309814
step: 620 | train loss: 0.009768720716238022 | train acc 0.7437673211097717
step: 630 | train loss: 0.008167452178895473 | train acc 0.7753720879554749
step: 640 | train loss: 0.010569467209279537 | train acc 0.7018284201622009
step: 650 | train loss: 0.009648664854466915 | train acc 0.7346101403236389
step: 660 | train loss: 0.00891639944165945 | train acc 0.7410959005355835
step: 670 | train loss: 0.010241778567433357 | train acc 0.7472677230834961
step: 680 | train loss: 0.00834228377789259 | train acc 0.7790055274963379
step: 690 | train loss: 0.00950560625642538 | train acc 0.7452055215835571
step: 700 | train loss: 0.008358567953109741 | train acc 0.7680965662002563
step: 710 | train loss: 0.010911940596997738 | train acc 0.7614213228225708
step: 720 | train loss: 0.009194972924888134 | train acc 0.7541666626930237
step: 730 | train loss: 0.010008937679231167 | train acc 0.7369146347045898
step: 740 | train loss: 0.00753216864541173 | train acc 0.7742364406585693
step: 750 | train loss: 0.0074683078564703465 | train acc 0.8119890689849854
step: 760 | train loss: 0.007779658772051334 | train acc 0.7761394381523132
step: 770 | train loss: 0.010380273684859276 | train acc 0.7666126489639282
step: 780 | train loss: 0.009216515347361565 | train acc 0.7413073778152466
step: 790 | train loss: 0.008828227408230305 | train acc 0.7686980366706848
step: 800 | train loss: 0.008949660696089268 | train acc 0.7660738825798035
step: 810 | train loss: 0.008083739317953587 | train acc 0.7732793092727661
step: 820 | train loss: 0.009268870577216148 | train acc 0.7395542860031128
step: 830 | train loss: 0.008049191907048225 | train acc 0.7890961170196533
step: 840 | train loss: 0.01034442987293005 | train acc 0.7301587462425232
step: 850 | train loss: 0.008401920087635517 | train acc 0.7549407482147217
step: 860 | train loss: 0.008670150302350521 | train acc 0.7779291272163391
step: 870 | train loss: 0.008343159221112728 | train acc 0.775564432144165
step: 880 | train loss: 0.008723469451069832 | train acc 0.7800546288490295
step: 890 | train loss: 0.008789326064288616 | train acc 0.7592339515686035
step: 900 | train loss: 0.008391674607992172 | train acc 0.7857142686843872
step: 910 | train loss: 0.0022509293630719185 | train acc 0.9753694534301758
step: 920 | train loss: 0.0034459128510206938 | train acc 0.936974823474884
step: 930 | train loss: 0.002649522153660655 | train acc 0.9513213038444519
step: 940 | train loss: 0.0026700717862695456 | train acc 0.9436813592910767
step: 950 | train loss: 0.002208288526162505 | train acc 0.9588477611541748
step: 960 | train loss: 0.0017977980896830559 | train acc 0.9533607959747314
step: 970 | train loss: 0.002635184908285737 | train acc 0.9297752976417542
step: 980 | train loss: 0.0027537813875824213 | train acc 0.9336219429969788
step: 990 | train loss: 0.002311831573024392 | train acc 0.9511172771453857
step: 1000 | train loss: 0.00230033160187304 | train acc 0.9467376470565796
step: 1010 | train loss: 0.0026346654631197453 | train acc 0.9300699830055237
step: 1020 | train loss: 0.0023879893124103546 | train acc 0.9398396015167236
step: 1030 | train loss: 0.002223898423835635 | train acc 0.9583333134651184
step: 1040 | train loss: 0.002487348625436425 | train acc 0.9513677954673767
step: 1050 | train loss: 0.0026320929173380136 | train acc 0.9364641308784485
step: 1060 | train loss: 0.0034305385779589415 | train acc 0.9201102256774902
step: 1070 | train loss: 0.0019913124851882458 | train acc 0.9546666741371155
step: 1080 | train loss: 0.0017986369784921408 | train acc 0.9571045637130737
step: 1090 | train loss: 0.002153924899175763 | train acc 0.9353970289230347
step: 1100 | train loss: 0.002072072820737958 | train acc 0.9509658217430115
step: 1110 | train loss: 0.0026295343413949013 | train acc 0.9359331130981445
step: 1120 | train loss: 0.002332839649170637 | train acc 0.9376730918884277
step: 1130 | train loss: 0.0028478691820055246 | train acc 0.9251700639724731
step: 1140 | train loss: 0.0021919782739132643 | train acc 0.9447514414787292
step: 1150 | train loss: 0.0017394189490005374 | train acc 0.960544228553772
step: 1160 | train loss: 0.0022927543614059687 | train acc 0.9459459781646729
step: 1170 | train loss: 0.0021220322232693434 | train acc 0.956204354763031
step: 1180 | train loss: 0.0017424121033400297 | train acc 0.9592640995979309
step: 1190 | train loss: 0.0024097857531160116 | train acc 0.9501384496688843
step: 1200 | train loss: 0.0024197485763579607 | train acc 0.9347826242446899
step: 1210 | train loss: 0.0017950964393094182 | train acc 0.9522510170936584
step: 1220 | train loss: 0.002102313796058297 | train acc 0.945983350276947
step: 1230 | train loss: 0.0022195158526301384 | train acc 0.9452887773513794
step: 1240 | train loss: 0.002910385839641094 | train acc 0.9254570603370667
step: 1250 | train loss: 0.0030574665870517492 | train acc 0.927496612071991
step: 1260 | train loss: 0.0017545222071930766 | train acc 0.9535210728645325
step: 1270 | train loss: 0.0030646538361907005 | train acc 0.9347826242446899
step: 1280 | train loss: 0.003248244524002075 | train acc 0.9223301410675049
step: 1290 | train loss: 0.0022312451619654894 | train acc 0.948051929473877
step: 1300 | train loss: 0.0029382307548075914 | train acc 0.9367977380752563
step: 1310 | train loss: 0.002431127242743969 | train acc 0.94141685962677
step: 1320 | train loss: 0.0015768230659887195 | train acc 0.9624161720275879
step: 1330 | train loss: 0.0030748401768505573 | train acc 0.9386084675788879
step: 1340 | train loss: 0.0028405466582626104 | train acc 0.9310829639434814
step: 1350 | train loss: 0.0019219382666051388 | train acc 0.9539918303489685
step: 1360 | train loss: 0.0029708684887737036 | train acc 0.9393019676208496
step: 1370 | train loss: 0.0024124872870743275 | train acc 0.9461326599121094
step: 1380 | train loss: 0.002503954106941819 | train acc 0.935350775718689
step: 1390 | train loss: 0.0017826262628659606 | train acc 0.9573590159416199
step: 1400 | train loss: 0.0019813126418739557 | train acc 0.9523809552192688
step: 1410 | train loss: 0.0021478631533682346 | train acc 0.9456824064254761
step: 1420 | train loss: 0.0018657614709809422 | train acc 0.9612518548965454
step: 1430 | train loss: 0.002645357046276331 | train acc 0.9431487321853638
step: 1440 | train loss: 0.0027903199661523104 | train acc 0.9256662130355835
step: 1450 | train loss: 0.002371148904785514 | train acc 0.9442934989929199
step: 1460 | train loss: 0.0019199308007955551 | train acc 0.9508426785469055
step: 1470 | train loss: 0.0017976867966353893 | train acc 0.954239547252655
step: 1480 | train loss: 0.002511087106540799 | train acc 0.9360222816467285
step: 1490 | train loss: 0.004092446994036436 | train acc 0.9134897589683533
step: 1500 | train loss: 0.0026047176215797663 | train acc 0.9273973107337952
step: 1510 | train loss: 0.0017092705238610506 | train acc 0.9513514041900635
step: 1520 | train loss: 0.0022099451161921024 | train acc 0.9480873942375183
step: 1530 | train loss: 0.002699580043554306 | train acc 0.941644549369812
step: 1540 | train loss: 0.0028289277106523514 | train acc 0.9283784031867981
step: 1550 | train loss: 0.0017746363300830126 | train acc 0.9630224704742432
step: 1560 | train loss: 0.0023932058829814196 | train acc 0.9400278925895691
step: 1570 | train loss: 0.002913114847615361 | train acc 0.9310829639434814
step: 1580 | train loss: 0.0024084150791168213 | train acc 0.9399726986885071
step: 1590 | train loss: 0.0025206825230270624 | train acc 0.9392712116241455
step: 1600 | train loss: 0.002723696641623974 | train acc 0.9336099624633789
step: 1610 | train loss: 0.0018485590117052197 | train acc 0.9413332939147949
step: 1620 | train loss: 0.002242671325802803 | train acc 0.9511811137199402
step: 1630 | train loss: 0.0033685157541185617 | train acc 0.916893720626831
step: 1640 | train loss: 0.0026119998656213284 | train acc 0.9329758882522583
step: 1650 | train loss: 0.0018832305213436484 | train acc 0.9544845819473267
step: 1660 | train loss: 0.001812996226362884 | train acc 0.9595141410827637
step: 1670 | train loss: 0.0019161991076543927 | train acc 0.9514563679695129
step: 1680 | train loss: 0.002305899281054735 | train acc 0.949367105960846
step: 1690 | train loss: 0.0019244843861088157 | train acc 0.9513888955116272
step: 1700 | train loss: 0.0018610545666888356 | train acc 0.9637584090232849
step: 1710 | train loss: 0.0024371964391320944 | train acc 0.9314888119697571
step: 1720 | train loss: 0.0019924496300518513 | train acc 0.9580419659614563
step: 1730 | train loss: 0.0022926267702132463 | train acc 0.9298245310783386
step: 1740 | train loss: 0.0021580092143267393 | train acc 0.944847583770752
step: 1750 | train loss: 0.0032206675969064236 | train acc 0.9228650331497192
step: 1760 | train loss: 0.0027377454098314047 | train acc 0.9382715821266174
step: 1770 | train loss: 0.002010652329772711 | train acc 0.9484126567840576
step: 1780 | train loss: 0.002099802019074559 | train acc 0.949860692024231
step: 1790 | train loss: 0.0015928036300465465 | train acc 0.9658469557762146
step: 1800 | train loss: 0.0021140959579497576 | train acc 0.945983350276947
step: 1810 | train loss: 0.0019351805094629526 | train acc 0.9639249444007874
***** Running evaluation *****
  Batch size = 8
***** Valid Eval results *****
  global_step = 1814
  valid_eval_accuracy = 0.7772505770710438
  valid_eval_loss = 0.758124779252445
